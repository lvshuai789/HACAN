config:
  activation: leaky_relu
  batch_size: 64
  checkpoint: /data3/lihaoxuan/New_Time/TKDE/github/runs/i2t_freeze/epoch=68-step=172499-v1.ckpt
  data_root: /home/ls/dataset/
  datasets: f30k
  decay_power: 1
  direction: t2i
  drop_rate: 0.1
  embed_dim: 768
  end_lr: 0
  exp_name: finetune_irtr_f30k
  experiment_name: ''
  fast_dev_run: false
  focal_type: prob
  fold5: false
  get_recall_metric: false
  hidden_size: 768
  image_only: false
  image_size: 224
  input_image_embed_size: 1024
  input_text_embed_size: 768
  lambda_softmax: 9
  learning_rate: 0.0001
  load_path: ''
  log_dir: result
  loss: GCD
  loss_names:
    contras: 0
    irtr: 1
    itm: 0
    mlm: 0
    mpp: 0
    nlvr2: 0
    snli: 0
    vcr: 0
    vcr_qar: 0
    vqa: 0
  lr_mult_cross_modal: 5
  lr_mult_head: 5
  lr_update: 10
  margin: 0.2
  max_epoch: 30
  max_steps: null
  max_text_len: 32
  mlm_prob: 0.15
  mlp_ratio: 4
  num_gpus: 1
  num_heads: 12
  num_layers: 6
  num_nodes: 1
  num_top_layer: 6
  num_workers: 8
  optim_type: adamw
  patch_size: 32
  per_gpu_batchsize: 64
  precision: 16
  resume_from: null
  save_path: ''
  seed: 0
  test_only: false
  tokenizer: /home/ls/bert-base-uncased
  val_check_interval: 1.0
  vit: swin_base_patch4_window7_224_in22k
  vocab_size: 30522
  warmup_steps: 10000
  weight_decay: 0.01
  whole_word_masking: false
